{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[1 0 1 ... 7 6 9]\n",
      "29399\n",
      "(29399, 784)\n",
      "29399\n",
      "(12601, 784)\n",
      "(29399,)\n",
      "(12601,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/ace/vertika/test/train.csv\")\n",
    "\n",
    "\n",
    "train_x = data.values[:,1:]\n",
    "train_y = data.values[:,0]\n",
    "print(train_x)\n",
    "print(train_y)\n",
    "split_size = int(train_x.shape[0]*0.7)\n",
    "print(split_size)\n",
    "\n",
    "train_x, val_x = train_x[:split_size, : ], train_x[split_size: , :]\n",
    "print(train_x.shape)\n",
    "print(len(train_x))\n",
    "print(val_x.shape)\n",
    "\n",
    "train_y, val_y = train_y[:split_size], train_y[split_size:]\n",
    "print(train_y.shape)\n",
    "print(val_y.shape)\n",
    "\n",
    "test_x = pd.read_csv( \"/home/ace/vertika/test/test.csv\"  ).values\n",
    "test_x.shape\n",
    "\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=28\n",
    "num_units=128        #hidden LSTM units\n",
    "n_input=28           #rows of 28 pixels\n",
    "learning_rate=0.001  #learning rate for adam\n",
    "n_classes=10         #mnist is meant to be classified in 10 classes(0-9).\n",
    "batch_size=256       #size of batch\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot\n",
    "\n",
    "def preproc(unclean_batch_x):\n",
    "    \"\"\"Convert values to range 0-1\"\"\"\n",
    "    temp_batch = unclean_batch_x / unclean_batch_x.max()\n",
    "    \n",
    "    return temp_batch\n",
    "\n",
    "def batch_creator(batch_size, dataset_length, dataset_name):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    batch_mask = rng.choice(dataset_length, batch_size)\n",
    "    if dataset_name == 'train':\n",
    "        batch_x = train_x[batch_mask]\n",
    "        batch_x = preproc(batch_x)\n",
    "        batch_x = np.reshape(batch_x, ( batch_size, n_input, n_input))\n",
    "        \n",
    "        batch_y = train_y[batch_mask]\n",
    "        batch_y = dense_to_one_hot(batch_y)\n",
    "        return batch_x, batch_y\n",
    "    elif dataset_name == 'val':\n",
    "        batch_x = preproc(val_x)\n",
    "        batch_x = np.reshape(batch_x, ( batch_size, n_input, n_input))\n",
    "        \n",
    "        batch_y = val_y\n",
    "        batch_y = dense_to_one_hot(batch_y)\n",
    "        return batch_x, batch_y\n",
    "    else:\n",
    "        batch_x = preproc(test_x)\n",
    "        batch_x = np.reshape(batch_x, ( batch_size, n_input, n_input))\n",
    "        return batch_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 28, 28), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "x=tf.placeholder(\"float\",[None,time_steps,n_input])\n",
    "print(x)\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "print(y)\n",
    "\n",
    "input=tf.unstack(x ,time_steps,1)\n",
    "lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n",
    "\n",
    "#prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "logits = tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-29e87b436976>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y))\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Iter 0  Accuracy 0.0625  Loss 2.680824 Validation Accuracy:0.09316721\n",
      "Epoch:0 Iter 100  Accuracy 0.203125  Loss 2.107657 Validation Accuracy:0.22887072\n",
      "Epoch:5 Iter 0  Accuracy 0.3359375  Loss 1.844715 Validation Accuracy:0.34671852\n",
      "Epoch:5 Iter 100  Accuracy 0.3046875  Loss 1.8929927 Validation Accuracy:0.34814698\n",
      "Epoch:10 Iter 0  Accuracy 0.38671875  Loss 1.7637151 Validation Accuracy:0.37481153\n",
      "Epoch:10 Iter 100  Accuracy 0.37890625  Loss 1.668837 Validation Accuracy:0.37203395\n",
      "Epoch:15 Iter 0  Accuracy 0.3828125  Loss 1.6853924 Validation Accuracy:0.3869534\n",
      "Epoch:15 Iter 100  Accuracy 0.38671875  Loss 1.6745071 Validation Accuracy:0.3894929\n",
      "28000\n",
      "Tensor(\"ArgMax_2:0\", shape=(?,), dtype=int64)\n",
      "len of pred:28000\n",
      "File written\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize variables\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for ep in range(epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = len(train_x)/batch_size\n",
    "        \n",
    "        for bt in range(total_batch):\n",
    "            batch_x, batch_y = batch_creator( batch_size, len(train_x), 'train')\n",
    "            _, c = sess.run( [ opt, loss ] ,  feed_dict = { x:batch_x, y: batch_y } )\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "            if ep %5 == 0:\n",
    "                if bt %100==0:\n",
    "                    acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n",
    "                    los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n",
    "\n",
    "                    #calculating test accuracy\n",
    "                    batch_x, batch_y = batch_creator( len(val_x) , len(val_x), 'val')\n",
    "                    print(\"Epoch:\" + str(ep) + \" Iter \"+ str(bt) + \"  Accuracy \"+ str(acc) +  \"  Loss \"+ str(los)  + \" Validation Accuracy:\" + str(sess.run(accuracy, feed_dict={x:batch_x, y: batch_y})))\n",
    "\n",
    "    \n",
    "    \n",
    "    batch_x = batch_creator( len(test_x) , len(test_x), 'test')\n",
    "    print(len(batch_x))\n",
    "    predict = tf.argmax(prediction, 1)\n",
    "    print(predict)\n",
    "    pred = predict.eval({x: batch_x})\n",
    "    \n",
    "    print(\"len of pred:\" + str(len(pred)))\n",
    "    \n",
    "    outFile = open(\"/home/ace/vertika/test/out.csv\", \"w\")\n",
    "    outFile.write(\"ImageId,Label\\n\")\n",
    "    for i in range( len(pred)):\n",
    "        outFile.write( str(i+1) + \",\" + str(pred[i]) + \"\\n\")\n",
    "        \n",
    "    print(\"File written\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ace_env2",
   "language": "python",
   "name": "ace_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
