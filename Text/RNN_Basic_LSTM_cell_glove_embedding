{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove vector\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading glove vector\")\n",
    "\n",
    "#Load GLOVE vectors\n",
    "filepath_glove = \"/home/ace/QANet/datasets/glove/glove.840B.300d.txt\"\n",
    "glove_vocab = []\n",
    "embedding_dict = {}\n",
    "\n",
    "content = open(filepath_glove,\"r\").readlines()\n",
    "#print(len(content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('content', 2196017)\n"
     ]
    }
   ],
   "source": [
    "print(\"content\", len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [',', '-0.082752', '0.67204', '-0.14987', '-0.064983'], [-0.082752, 0.67204, -0.14987, -0.064983, 0.056491])\n",
      "(100000, ['apolitical', '0.33455', '0.15881', '0.65359', '0.65112'], [0.33455, 0.15881, 0.65359, 0.65112, -0.02487])\n",
      "(200000, ['fkin', '-0.11896', '-0.68071', '0.036711', '-0.29231'], [-0.11896, -0.68071, 0.036711, -0.29231, 0.11306])\n",
      "(300000, ['jeanie', '-0.040849', '0.41066', '0.031167', '-0.14727'], [-0.040849, 0.41066, 0.031167, -0.14727, -0.21513])\n",
      "(400000, ['Entwine', '-0.24037', '0.021773', '0.10478', '-0.15989'], [-0.24037, 0.021773, 0.10478, -0.15989, 0.15963])\n",
      "(500000, ['1924-1925', '0.73964', '-0.69671', '0.36413', '-0.7755'], [0.73964, -0.69671, 0.36413, -0.7755, -0.5716])\n",
      "(600000, ['Ski-doo', '1.0453', '-0.45921', '-0.74978', '-0.015926'], [1.0453, -0.45921, -0.74978, -0.015926, 0.1798])\n",
      "(700000, ['l\\xc3\\xa9gumes', '-0.50412', '0.22427', '0.52541', '-0.23137'], [-0.50412, 0.22427, 0.52541, -0.23137, -0.4313])\n",
      "(800000, ['TSUBASA', '-0.10387', '-0.33314', '0.1235', '0.52249'], [-0.10387, -0.33314, 0.1235, 0.52249, 0.89945])\n",
      "(900000, ['eftda', '-0.022242', '0.54698', '0.13548', '-0.94522'], [-0.022242, 0.54698, 0.13548, -0.94522, 0.41188])\n",
      "(1000000, ['Crowd-Sourced', '0.20899', '-0.91185', '-0.47795', '0.78695'], [0.20899, -0.91185, -0.47795, 0.78695, -0.61393])\n",
      "(1100000, ['Abnoraml', '0.34347', '-0.0016653', '-0.048951', '0.83907'], [0.34347, -0.0016653, -0.048951, 0.83907, -0.64025])\n",
      "(1200000, ['NewsToob', '-0.017412', '-0.631', '0.16732', '0.2209'], [-0.017412, -0.631, 0.16732, 0.2209, -0.23861])\n",
      "(1300000, ['fuen', '0.45125', '-0.557', '0.19345', '0.31494'], [0.45125, -0.557, 0.19345, 0.31494, -0.60439])\n",
      "(1400000, ['Arag\\xc3\\xb3', '0.077455', '0.45315', '-0.33474', '-0.28624'], [0.077455, 0.45315, -0.33474, -0.28624, -0.13028])\n",
      "(1500000, ['OC-X', '0.23382', '0.456', '0.41497', '-0.15525'], [0.23382, 0.456, 0.41497, -0.15525, 0.25349])\n",
      "(1600000, ['syarat', '0.70508', '-0.32466', '-0.026753', '0.37498'], [0.70508, -0.32466, -0.026753, 0.37498, 0.40483])\n",
      "(1700000, ['\\xc4\\x8dia', '-0.066328', '-0.59288', '0.91445', '0.44067'], [-0.066328, -0.59288, 0.91445, 0.44067, 0.62964])\n",
      "(1800000, ['Mchedlishvili', '-0.17037', '-0.58998', '0.35532', '-0.43737'], [-0.17037, -0.58998, 0.35532, -0.43737, 0.16677])\n",
      "(1900000, ['00:29:24', '0.31228', '-0.46651', '0.16182', '0.13687'], [0.31228, -0.46651, 0.16182, 0.13687, -0.26509])\n",
      "(2000000, ['Blogger/Google', '0.65916', '-0.23847', '0.035889', '0.14653'], [0.65916, -0.23847, 0.035889, 0.14653, -0.19955])\n",
      "(2100000, ['ItPrice', '0.30914', '-1.0883', '1.0113', '-0.63131'], [0.30914, -1.0883, 1.0113, -0.63131, -1.0943])\n"
     ]
    }
   ],
   "source": [
    "for ix,line in enumerate(content):\n",
    "    line = line.strip().split(' ')\n",
    "    \n",
    "    glove_vocab.append(line[0])\n",
    "    glove_emb = [float(x) for x in line[1:] ]\n",
    "    if ix%100000 == 0:\n",
    "        print(ix, line[:5], glove_emb[:5])\n",
    "    embedding_dict[line[0]] = glove_emb \n",
    "    \n",
    "emb_dim =len(glove_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_vocab\n",
      "(0, ',')\n",
      "(1, '.')\n",
      "(2, 'the')\n",
      "(3, 'and')\n",
      "(4, 'to')\n",
      "(5, 'of')\n",
      "(6, 'a')\n",
      "(7, 'in')\n",
      "(8, '\"')\n",
      "(9, ':')\n",
      "embedding_dict\n",
      "('AscensionMidkemia', [-0.60919, -0.29819, -0.42569, 0.14072, -0.092037, 0.18863, -0.65761, 0.2186, 0.77986, -1.2858, 0.51591, -0.68246, -0.84942, 0.36792, 0.14413, 0.211, 0.22729, -0.85337, 0.63919, -0.007462, 0.21365, 0.18371, -0.33559, -0.20532, 0.14996, 0.076865, 0.15038, 0.14943, -0.3188, 0.022243, -0.09557, 0.10571, 0.028096, -0.54589, -0.23022, 0.22428, -0.33524, 0.0035641, -0.26515, -0.19595, -0.039068, -0.16827, 0.22675, 0.23383, 0.15794, -0.44057, 0.022638, 0.054218, -0.14076, -0.36613, -0.35779, -0.41895, -0.10663, -0.51519, 0.32525, 0.61571, 0.07854, 0.015417, -0.047197, -0.79469, 0.47842, -0.41728, 0.25492, -0.99737, 0.32151, 0.35743, -0.053391, 0.22092, 0.36775, -0.27624, 0.11147, -0.20539, -0.17055, 0.20146, 0.29752, 0.15177, 0.53257, 0.37704, 0.079853, -0.0091699, 0.24116, 0.017176, -0.13942, -0.12497, 0.26717, 0.27878, -0.28577, 0.56784, -0.24924, -0.025449, 0.20394, -0.42446, 0.10462, -0.10965, -0.3248, 0.56995, 0.52578, 0.59841, -0.1805, -0.067358, 0.080473, 0.057377, 0.7078, 0.086135, 0.17578, 0.51651, -0.48427, -0.21125, -0.58349, -0.048048, 0.17019, 0.34432, -0.61781, -0.021516, 0.31058, 0.37524, 0.41687, -0.15479, 0.05936, 0.1255, -0.57457, 0.058702, 0.084275, -0.18782, 0.49009, -0.086735, 0.19523, 0.51238, -0.10016, 0.025231, 0.1419, -0.11948, -0.085772, 0.092083, 0.22379, -0.55004, -0.40849, -0.10261, -0.00070936, 0.18483, 1.3396, -0.37817, -0.10977, 0.21373, -0.30104, 0.27156, -0.314, 0.042899, -0.38035, -0.034718, 0.27861, -0.73495, -0.24522, -0.36233, -0.030548, 0.495, 0.37785, 0.47356, 0.17199, 0.18282, 0.4914, -0.023001, -0.066334, 0.43013, 0.1843, 0.31764, 0.155, 0.2093, -0.53823, -0.42423, -0.040292, -0.30076, 0.39457, 0.17377, -0.25286, 0.06112, 0.26323, 0.057427, 0.5719, 0.12694, -0.0028999, 0.29729, -0.0044101, 0.47636, -0.25462, -0.025598, 0.023133, 0.01352, 0.13829, 0.048599, -0.26273, -0.22663, -0.12751, -0.43582, 0.24617, -0.55117, 0.19177, 0.53984, -0.055382, 0.1317, 0.042553, -0.18194, 0.53325, 0.097964, -0.44568, -0.19268, -0.43619, 0.10226, -0.091638, -0.40654, 0.4671, -0.40771, 0.18269, -0.36971, -0.13432, -0.48361, -0.087244, -0.038656, -0.18168, -0.16148, -0.57753, -0.13591, 0.50111, -0.24738, -0.50555, 0.27912, 0.12341, -0.24176, 0.134, -0.36554, 0.019919, 0.55741, -0.024052, -0.3653, 0.25325, -0.035583, 0.04911, -0.12997, 0.50137, -0.30892, -0.12923, -0.12486, 0.0079577, -0.16182, 0.066001, -0.012511, 0.0081884, 0.22568, 0.21084, -0.2475, 0.32474, -0.1792, 0.19921, 0.19809, -0.15563, -0.33749, -0.30501, -0.07869, -0.52727, 0.0060786, 0.47208, 0.032782, -0.25951, -0.04523, 0.076738, -0.42155, 0.58248, 0.12951, -0.68882, 0.13808, 0.70641, 0.42113, -0.28523, 0.74304, 0.50426, -0.12282, -0.50035, -0.10923, -0.063231, -0.034483, -0.75888, 0.19436, 0.2274, 0.48536, 0.38753, 0.42543, 0.24409, 0.38907, -0.4905, -0.021568, 0.52337, -0.12939, -0.064431, 0.48227, 0.009308, 0.053685, -0.44072, 0.053427, 0.17876, 0.13729])\n",
      "('Inculcation', [0.47866, 0.42564, -0.10835, 0.78806, -0.49926, 0.20725, -0.16962, 0.25107, 0.54671, -1.5378, 0.43413, 0.27094, 0.29468, 0.30273, 1.1665, -0.28558, -0.16556, -1.5719, 0.45495, 0.36743, 0.16255, 0.082178, -0.24196, -0.15645, 0.41186, 0.20732, -0.44502, -0.15016, -0.31533, 0.33988, -0.09917, -0.25021, -0.33268, 0.17205, -0.32967, -0.50938, -0.069844, -0.099582, -0.024455, 0.10995, -0.18722, -0.30104, -0.8377, 0.045479, 0.26319, -0.068238, -0.33452, 0.14449, 0.057227, 0.75383, 0.30031, 0.0054737, -0.3674, -0.10049, 0.12496, -0.9042, 0.3289, 0.51184, -0.18651, -0.55046, 0.6266, 0.72766, -0.32562, -1.2387, -0.23146, -0.7159, 0.053877, -0.21106, -0.075795, -0.50171, -0.11032, 0.11263, 0.31638, -0.0091328, 0.30126, -0.65095, -0.23651, 0.64476, 0.057114, 0.08209, 0.62148, 0.15866, -0.36618, 0.55655, 0.16747, 0.8422, 0.41375, 0.16943, -0.96758, 0.0043011, -0.16846, -0.030744, 0.55859, -0.34326, 0.35902, 1.1719, -0.41962, -0.08601, -0.53074, 0.28024, -0.074576, 0.23815, 0.24319, 0.78569, -0.41437, -0.23065, 0.023577, -0.2523, -0.52654, -0.028123, 0.12567, 0.68775, -0.22012, -0.027217, 0.041402, 0.5551, -0.33744, -0.17535, -0.17312, -0.26316, 0.56839, -0.046992, -0.39193, 0.67762, 0.6762, 0.26293, -0.35539, 0.67516, -0.47048, -0.080939, 0.23866, 0.50699, -0.55123, 0.14491, -0.17945, -0.25168, 0.12519, -0.27452, 0.46058, 0.054219, 1.0557, 0.32218, -0.54492, -0.52192, 0.037545, 0.35508, 0.16964, 0.53772, -0.03353, 0.1188, -0.41435, -0.68559, 0.68172, 0.0048706, 0.45195, -0.23272, -0.040975, -0.020047, 0.54287, -0.45142, 0.35675, 0.4345, 0.3415, 1.4537, 0.47513, -0.29898, -0.17117, 0.26879, -0.25657, -0.17008, -0.12741, 0.16767, 0.020717, -0.077906, 0.57373, -0.21804, -0.19531, 0.46444, -0.018886, -0.31316, 0.15329, -1.1219, 0.061451, 0.073601, -0.18539, -0.51553, 0.4163, -0.34012, 0.23928, -0.20353, -0.36137, 0.33115, 0.35462, -0.46397, -0.39683, 0.51036, -0.047623, -0.096992, 0.25322, -0.8709, 0.55548, 0.20895, -0.065223, -0.17596, -1.1357, -0.066059, 0.69181, -0.15039, 0.24461, 0.12058, 0.014142, 0.099729, -0.51648, -0.60482, -0.2797, -0.55996, 0.32651, 0.25151, 0.908, 0.16079, 0.07682, 0.15338, 0.1434, -0.28035, -0.34583, -0.32447, 0.64687, 1.1324, 0.57211, -0.35966, 0.64709, 0.53232, -0.55879, -0.27924, -0.15406, 0.1593, 0.21216, 0.10427, 0.61151, 0.45386, -1.2613, -0.14676, -0.42592, -0.30466, 0.080242, 1.1325, -0.44092, -0.32485, -0.18853, -0.38718, -0.11065, 0.59306, 0.37724, -0.12424, -0.37386, -0.72131, 0.31853, 0.43877, 0.15948, -0.46712, -0.17181, -0.78741, -0.56782, -0.22956, -0.050468, 0.45233, -0.27572, -0.36279, 0.12286, 0.57398, 0.024614, -0.083392, 0.0023604, -0.020385, 0.28217, -0.23203, -0.28477, 0.2803, 0.024917, -0.35336, -0.056361, -0.72366, -0.5233, 0.53408, 0.43514, -0.51467, -0.025358, -0.0093041, 0.063325, -0.687, 0.38587, -0.62702, -0.28504, -0.17527, 0.72794, 0.69233, -0.18765, 0.62389, -0.053413, -0.70628])\n",
      "('head.As', [0.69898, -0.44982, -0.34132, 0.25586, -0.058532, 0.40206, -0.29757, 0.26346, -0.044741, -1.1859, 0.19518, -0.56781, -0.14712, 0.18377, 0.45928, -0.11322, 0.033024, -1.7401, 0.15795, -0.12825, 0.090899, 0.8444, 0.14874, -0.29222, -0.27267, -0.41247, -0.36619, -0.28668, -0.23925, -0.23831, 0.27737, -0.33851, 0.17349, 0.22471, -0.49095, 0.093362, -0.49455, 0.17006, 0.27127, 0.14652, 0.49621, -0.61856, -0.23361, 0.033168, -0.042798, 0.18668, 0.17684, 0.023484, 0.2057, -0.31177, 0.24731, 0.38609, -0.32075, 0.89576, 0.3602, -0.86104, -0.21692, 0.03906, 0.40846, -0.19331, -0.14084, 0.46062, -0.049389, -0.13658, 0.19818, 0.24245, -0.21185, 0.17974, 0.97142, -0.28789, -0.08276, -0.13186, -0.23111, 0.24931, -0.45716, 0.31429, -0.24096, -0.072824, 0.032611, 0.25008, -0.65937, 0.075245, -0.20675, 0.081721, 0.12591, 0.59071, 0.92222, -0.13416, -0.34302, -0.12634, 0.52479, -0.63536, 0.85785, 0.89558, -0.32671, 0.33177, 0.21089, -0.32655, 0.17129, 0.35553, -0.23634, -0.23623, 0.14368, -0.0022461, 0.2611, -1.5088, -0.18286, -0.224, -0.44263, -0.74786, -0.70923, 0.35183, 0.10577, -0.16687, 0.47533, -0.015938, 0.156, 0.055836, 0.30476, 0.57177, 0.13034, -0.01208, -0.20084, 0.45134, 0.21981, 0.31642, 0.068563, -0.26014, 0.53997, -0.058329, -0.1871, -0.52542, 0.30501, -0.015602, 0.28184, -0.079464, -0.016192, 0.087185, -0.067502, -0.0090154, 0.5493, -0.0056887, -0.42267, 0.01866, -0.42913, -0.28378, 0.30746, -0.052759, 0.33143, 0.17349, -0.39156, -0.13778, 0.10137, -0.41268, -0.068915, -0.0022809, 0.62145, 0.065373, 0.31261, -0.28315, -0.016315, 0.078278, -0.40705, 0.20816, 0.15586, -0.60064, 0.21801, 0.34066, 0.33611, -0.32561, -0.48375, -0.61912, 0.51549, -0.43856, 0.31313, 0.24927, -0.58591, 0.11332, -0.62322, -0.060558, -0.2999, -0.18363, 0.16857, 0.27049, 0.26918, 0.19671, 0.32213, -0.53268, 0.050046, 0.035174, 0.4315, 0.36517, 0.25571, 0.012133, -0.20071, -0.42387, 0.23406, -0.29544, 0.31695, -0.51481, -0.50311, 0.66744, -0.7188, 0.36508, -0.23179, 0.37288, 0.51879, 0.050414, -0.35313, 0.18645, -0.39214, -0.11279, -0.29382, 0.33769, 0.041064, -0.25233, 0.42984, 0.013889, 0.90121, -0.22432, 0.048817, -0.54254, -0.13164, -0.69425, -0.45266, -0.45685, 0.36313, -0.29208, -0.70137, 0.29508, -0.27174, 0.22762, -0.32586, 0.22397, -0.50009, -0.30287, -0.44427, 0.39703, 0.074578, 0.19448, 0.1753, 0.025565, 0.10614, 0.095748, 0.37879, 0.56673, 0.69292, -0.0064593, 0.43769, 0.45118, -0.70579, -0.24236, 0.08596, 0.40597, 0.036144, -0.73127, -0.12148, 0.81158, -0.10126, 0.155, 0.26741, -0.12361, 0.040813, -0.16419, -0.26229, 0.48508, -0.17662, -0.95779, -0.87688, 0.42229, -0.0062737, -0.33301, -0.38993, -0.084677, -0.043737, 0.97286, -0.081078, -0.4924, -0.38655, -0.70993, -0.45185, 0.19407, -0.699, -0.33032, 0.12854, 0.34138, -0.37106, -0.02063, -0.41981, 0.044846, -0.39801, -0.62846, 0.37144, -0.45193, 0.31108, 0.030675, 0.2254, 0.77258, 0.023997, -0.79516])\n",
      "('6-night', [0.18484, -0.68836, 0.53384, 0.26941, 0.1885, 0.020713, 0.084762, -1.0148, 0.16232, -1.1395, 0.37783, 0.84979, -0.30645, 0.65255, 1.1917, -0.9391, -0.050485, -1.105, 0.34654, 0.074392, 0.23591, 0.72776, 0.33807, 0.8666, -0.078499, -0.0046333, -0.10955, 0.083358, -0.66045, 1.3327, 0.62587, -0.38466, 0.3502, -0.11814, -0.0237, 0.048257, 0.3439, -0.30922, -0.10905, 0.022838, 0.025224, -0.35211, -0.98343, -0.22251, 0.27866, 0.06637, -0.12401, 0.8399, -0.49654, 0.63096, 0.0020967, -0.070839, -0.17187, -0.87377, -0.1535, 0.89862, 0.42019, 0.73452, -0.07976, 0.37283, 0.7319, -0.082406, 0.0077057, -0.75876, 0.14564, 0.151, -0.40902, 0.11579, -0.41691, 0.14417, 0.16201, 0.76961, -0.13104, -0.25376, 0.0013922, -0.62592, -0.41749, 0.81563, -0.54789, 0.19645, -0.51568, -0.36253, 0.18591, 0.56403, 0.55013, 0.42194, -0.055183, -0.21203, 0.24515, -0.78696, 0.20817, 0.95709, 0.26797, 0.58835, -0.12283, -0.2445, -0.18057, 0.5932, 0.76023, 0.17089, 0.1348, 0.2067, -0.6434, -0.1544, 0.02394, -1.0729, -0.55117, 0.10723, 0.58458, -0.42179, 0.80261, 0.38162, 0.0095464, 0.79552, 0.45416, -0.57672, 0.48855, 0.13499, -0.36267, -0.47519, 0.20652, -0.10597, 0.50012, 0.088858, -0.44875, 0.50338, -1.2917, 0.37798, 0.25427, 0.27491, 0.098373, -0.29362, -0.89659, 0.54918, 0.37069, 0.29973, 0.13249, 0.51846, -0.19469, -0.010743, 1.022, -0.63192, -0.31194, -0.44667, 0.31306, -0.034801, -0.29846, 0.020308, 0.4049, -0.22812, 0.4594, 0.26168, 0.0069337, -0.2754, -0.26578, 0.65185, 0.26778, -0.15082, 0.1593, 0.6492, 0.034496, 0.58326, -0.17144, 0.47301, 0.88105, 0.61138, 0.20426, 0.22759, 0.27736, -0.63155, -0.01523, 0.93062, -0.034154, -0.048493, 0.69371, 0.20869, -0.19854, -0.46525, -0.44292, -0.078062, 0.16695, -0.2062, -0.27327, 0.13156, 0.19421, -0.26466, 0.61404, 0.18635, -0.69009, -0.2668, -0.25679, -0.3889, -0.40918, -0.82167, 0.74921, 0.69132, -0.38074, 0.61945, 0.15002, -0.66123, -0.43326, -0.73416, -0.62273, 0.10156, 0.21591, -0.5231, -0.76347, -0.10116, 0.20299, 0.51197, -0.36858, 0.60509, 0.51456, 0.16458, 0.093078, 0.59615, 0.07607, 0.36864, 1.0987, 0.39683, -0.099785, -0.26373, 0.15845, -0.8627, -0.1561, -0.39432, -0.084188, -0.16556, -0.3855, 0.7398, 0.89948, -0.013358, 0.059874, -0.0041437, 0.043954, 0.49909, 0.351, -0.47329, 0.29505, -0.35486, -0.28357, -0.17029, -0.58337, -0.23204, 0.2703, 0.50878, -0.28176, 0.47388, 0.18883, -0.21991, -0.56641, 0.31953, -0.16387, -0.34452, -0.53144, 0.18886, -0.0044203, -1.0751, 0.38171, -0.30184, -0.3459, 0.22244, -0.7, -0.21564, 0.53988, 0.49793, 0.66857, 1.2444, -0.67146, 0.17152, 0.060597, -0.52863, 0.1472, 0.43574, 0.42208, 0.49011, -0.055492, 0.47258, 0.42966, 0.60672, -0.38292, -0.71819, -0.98232, 0.42439, 0.41831, 0.33024, -1.0944, 0.50357, -0.35231, 0.53198, 0.98919, 0.58964, -0.011189, -0.97309, -0.20007, 0.70861, -0.12439, -0.016078, -0.2454, -1.0532])\n",
      "(\"d'eux\", [0.10738, -0.41705, -0.61714, -0.13003, 0.036559, -0.6241, 0.59758, 0.063077, -0.283, -1.823, 0.86447, 0.46748, 0.7005, 0.20069, 0.33776, 0.37954, 0.16359, -1.0594, 0.46932, 0.54373, 0.53652, -0.019972, 0.19176, 0.6927, 0.7023, -0.3433, 0.50537, 0.060213, 0.32813, -0.021554, 0.5181, -0.99648, -0.07064, 0.076432, -0.17855, -0.32665, -0.028324, 0.62307, -0.42473, -0.13763, -0.78062, 0.26762, -0.62524, 0.57374, 0.014626, -0.63313, 0.44035, -0.55592, -0.12027, 1.4234, -0.32156, 0.13789, 0.7559, -0.17497, -0.59486, -0.90528, -0.33861, -0.060755, 0.48351, 0.47035, -0.12899, 0.085801, -0.13745, -0.22188, 0.24488, 0.36408, -0.10694, -0.53577, 0.36211, -0.59299, -0.20532, 0.32517, -0.023507, 0.35615, -0.31991, -0.37481, -0.36237, -0.10246, 0.43702, -0.80757, -0.38049, 0.2389, 0.1322, -0.054187, -0.25031, 0.012011, 0.74475, 0.068836, -0.67177, -0.16321, 0.33994, -0.61774, -0.17566, -0.0066742, -0.78291, 1.0125, -0.82142, -0.41086, -0.69752, 0.20016, 0.45882, -0.20793, 0.064447, -0.6938, -0.025583, -1.0715, 0.079975, -0.0092526, -0.19382, 1.0342, -0.14667, 0.072245, 0.21156, 0.092456, -0.19508, -0.0074145, -0.56498, -0.58529, 0.086226, 0.069202, -0.12064, 0.081408, 0.19974, 0.23096, -0.20159, -0.13709, 0.069192, 0.53511, 0.62335, -0.88566, 0.29567, -0.71871, 0.58831, 0.72883, -0.55619, -0.347, -0.31092, -0.095312, -0.22913, -0.034791, 0.66661, 0.12851, 0.20516, 0.13039, -0.075101, 0.30432, 0.48587, 0.029857, -1.1982, 0.08738, -0.57782, 0.86391, -0.088147, -0.14147, 0.33044, -0.078869, 0.66045, -0.042849, 0.043862, -0.023853, -0.080477, 0.25653, 0.24063, -0.016484, 0.58019, 0.22847, -0.36569, 0.78427, -0.43156, -0.60327, 0.13292, 0.34285, 0.37543, 0.59, 0.22548, -0.06613, -0.23188, 0.28524, -0.55241, -0.76395, -0.3731, 0.30756, 0.34149, -0.38603, 0.2804, -0.20731, 0.30322, 0.18144, 0.39493, -0.25854, -0.0078759, 0.16524, -0.06277, 0.080852, -0.23861, 0.091886, 0.033965, -0.88968, 1.064, -0.5968, 0.082684, 0.53136, -0.21077, -0.076251, -1.349, -0.24929, 0.33101, -0.14695, -0.13238, 0.53259, 0.79528, 0.0056562, 0.048043, 0.64301, -0.26892, 0.38882, 0.064686, -0.10644, 1.2967, -1.0683, 0.32512, 0.089789, -1.021, 0.18187, 0.31367, -0.24892, -0.41304, -0.98865, -0.41236, -0.13337, -0.14767, 0.046638, 0.31535, -0.17722, 0.077872, 0.06722, 0.29245, 0.36664, -0.53094, 0.15252, -0.032299, 0.51693, -0.56932, 0.015235, 0.077968, 0.2681, 0.81873, 0.63895, 0.40299, -0.27668, 0.40551, 0.36119, 0.37056, 0.058039, -0.44291, -0.015813, -0.88726, 0.39, 0.15187, -0.35642, -0.42985, -0.53285, -0.79357, -0.62062, 0.29439, 0.091236, -0.25644, -0.39434, -0.58576, -0.0761, -0.33555, -0.28643, 0.38856, -0.14271, -0.12343, 0.0040451, 0.34114, -0.22152, 0.28307, -0.0027049, 0.11133, -0.50937, -0.16081, 0.32045, 0.14951, 0.621, 0.22979, 0.43056, -0.31744, 0.23342, 0.62797, -0.50329, -0.41656, 0.506, 0.24857, 0.53734, 0.29032, -0.16335, 0.035124, -1.2628])\n",
      "('embedding dim', 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"glove_vocab\")\n",
    "for i, val in enumerate(glove_vocab[:10]):\n",
    "    print(i,val)\n",
    "\n",
    "print(\"embedding_dict\")\n",
    "\n",
    "tempDict = dict(embedding_dict.items()[0:5])\n",
    "for key in tempDict:\n",
    "    print(key,tempDict[key])\n",
    "print(\"embedding dim\" , emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "['long ago , the mice had a general council to consider what measures they could take to outwit their common enemy , the cat . some said this , and some said that but at last a young mouse got up and said he had a proposal to make , which he thought would meet the case . you will all agree , said he , that our chief danger consists in the sly and treacherous manner in which the enemy approaches us . now , if we could receive some signal of her approach , we could easily escape from her . i venture , therefore , to propose that a small bell be procured , and attached by a ribbon round the neck of the cat . by this means we should always know when she was about , and could easily retire while she was in the neighbourhood . this proposal met with general applause , until an old mouse got up and said  that is all very well , but who is to bell the cat ? the mice looked at one another and nobody spoke . then the old mouse said it is easy to propose impossible remedies .']\n",
      "['long', 'ago', ',', 'the', 'mice', 'had', 'a', 'general', 'council', 'to', 'consider', 'what', 'measures', 'they', 'could', 'take', 'to', 'outwit', 'their', 'common', 'enemy', ',', 'the', 'cat', '.', 'some', 'said', 'this', ',', 'and', 'some', 'said', 'that', 'but', 'at', 'last', 'a', 'young', 'mouse', 'got', 'up', 'and', 'said', 'he', 'had', 'a', 'proposal', 'to', 'make', ',', 'which', 'he', 'thought', 'would', 'meet', 'the', 'case', '.', 'you', 'will', 'all', 'agree', ',', 'said', 'he', ',', 'that', 'our', 'chief', 'danger', 'consists', 'in', 'the', 'sly', 'and', 'treacherous', 'manner', 'in', 'which', 'the', 'enemy', 'approaches', 'us', '.', 'now', ',', 'if', 'we', 'could', 'receive', 'some', 'signal', 'of', 'her', 'approach', ',', 'we', 'could', 'easily', 'escape', 'from', 'her', '.', 'i', 'venture', ',', 'therefore', ',', 'to', 'propose', 'that', 'a', 'small', 'bell', 'be', 'procured', ',', 'and', 'attached', 'by', 'a', 'ribbon', 'round', 'the', 'neck', 'of', 'the', 'cat', '.', 'by', 'this', 'means', 'we', 'should', 'always', 'know', 'when', 'she', 'was', 'about', ',', 'and', 'could', 'easily', 'retire', 'while', 'she', 'was', 'in', 'the', 'neighbourhood', '.', 'this', 'proposal', 'met', 'with', 'general', 'applause', ',', 'until', 'an', 'old', 'mouse', 'got', 'up', 'and', 'said', 'that', 'is', 'all', 'very', 'well', ',', 'but', 'who', 'is', 'to', 'bell', 'the', 'cat', '?', 'the', 'mice', 'looked', 'at', 'one', 'another', 'and', 'nobody', 'spoke', '.', 'then', 'the', 'old', 'mouse', 'said', 'it', 'is', 'easy', 'to', 'propose', 'impossible', 'remedies', '.']\n",
      "Loaded training data...\n"
     ]
    }
   ],
   "source": [
    "def read_data( train_file):\n",
    "    with open(train_file) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    print(np.shape(content))\n",
    "    print(content)\n",
    "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
    "    print(content)\n",
    "    content = np.array(content)\n",
    "    return content\n",
    "\n",
    "train_file = \"sample.txt\"\n",
    "training_data = read_data(train_file)\n",
    "print(\"Loaded training data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict using set\n",
      "{'all': 0, 'consider': 1, 'retire': 2, 'consists': 3, 'danger': 50, 'had': 6, 'young': 7, 'should': 8, 'to': 9, 'outwit': 10, 'easy': 11, 'take': 100, 'then': 12, 'means': 14, 'very': 15, 'propose': 16, 'know': 17, 'they': 18, 'now': 19, 'enemy': 20, 'always': 21, 'signal': 22, 'she': 36, 'small': 24, 'round': 25, 'old': 26, 'some': 27, 'escape': 28, 'our': 29, 'ribbon': 30, 'what': 31, 'said': 32, 'got': 33, 'approach': 34, '?': 35, 'common': 23, 'be': 37, 'we': 38, 'measures': 103, 'met': 40, 'approaches': 41, 'by': 42, 'about': 44, 'last': 45, 'her': 46, 'receive': 47, 'of': 48, 'could': 49, 'bell': 5, 'one': 4, 'another': 52, 'impossible': 53, 'from': 54, 'spoke': 55, 'would': 56, 'neck': 105, 'long': 57, '.': 58, 'their': 59, 'therefore': 60, 'proposal': 61, 'was': 62, 'until': 63, 'remedies': 64, 'a': 104, 'that': 66, 'treacherous': 67, 'nobody': 68, 'but': 69, 'mice': 70, 'general': 90, 'with': 72, ',': 51, 'he': 73, 'case': 74, 'sly': 75, 'this': 76, 'attached': 77, 'up': 78, 'us': 79, 'cat': 80, 'will': 81, 'while': 82, 'venture': 83, 'meet': 84, 'agree': 85, 'at': 91, 'and': 87, 'is': 88, 'it': 89, 'an': 71, 'manner': 86, 'council': 92, 'in': 93, 'mouse': 94, 'if': 95, 'thought': 108, 'make': 97, 'when': 98, 'procured': 99, 'neighbourhood': 43, 'which': 101, 'you': 102, 'who': 39, 'ago': 65, 'applause': 13, 'i': 106, 'well': 107, 'looked': 96, 'chief': 109, 'easily': 110, 'the': 111}\n",
      "('Vocab size', 112)\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary and reverse dictionary and return it\n",
    "def build_dataset_using_collections( data ):\n",
    "    counts = collections.Counter(data).most_common()  #using most_common() to get a list\n",
    "    #print(counts)\n",
    "    dictionary = dict()\n",
    "    \n",
    "    #creating word to id mapping\n",
    "    for word, _ in counts:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    return dictionary\n",
    "        \n",
    "def build_dataset( data ):\n",
    "    counts = list(set( data))\n",
    "    #print(counts)\n",
    "    dictionary = dict()\n",
    "    \n",
    "    #creating word to id mapping\n",
    "    for word in counts:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    reverse_dict = dict((dictionary[k], k) for k in dictionary)\n",
    "    return dictionary, reverse_dict\n",
    "\n",
    "#dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "dictionary,reverse_dict = build_dataset(training_data)\n",
    "print(\"Dict using set\")\n",
    "print(dictionary)\n",
    "\n",
    "\"\"\"\n",
    "#Getting same thing with list so why use collections\n",
    "dictioanry = build_dataset_using_collections(training_data)\n",
    "print(\"Dict using collections\")\n",
    "print(dictionary)\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = len(dictionary)\n",
    "print(\"Vocab size\" , vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('all', 0), ('consider', 1), ('retire', 2), ('consists', 3), ('one', 4), ('bell', 5), ('had', 6), ('young', 7), ('should', 8), ('to', 9), ('outwit', 10), ('easy', 11), ('then', 12), ('applause', 13), ('means', 14), ('very', 15), ('propose', 16), ('know', 17), ('they', 18), ('now', 19), ('enemy', 20), ('always', 21), ('signal', 22), ('common', 23), ('small', 24), ('round', 25), ('old', 26), ('some', 27), ('escape', 28), ('our', 29), ('ribbon', 30), ('what', 31), ('said', 32), ('got', 33), ('approach', 34), ('?', 35), ('she', 36), ('be', 37), ('we', 38), ('who', 39), ('met', 40), ('approaches', 41), ('by', 42), ('neighbourhood', 43), ('about', 44), ('last', 45), ('her', 46), ('receive', 47), ('of', 48), ('could', 49), ('danger', 50), (',', 51), ('another', 52), ('impossible', 53), ('from', 54), ('spoke', 55), ('would', 56), ('long', 57), ('.', 58), ('their', 59), ('therefore', 60), ('proposal', 61), ('was', 62), ('until', 63), ('remedies', 64), ('ago', 65), ('that', 66), ('treacherous', 67), ('nobody', 68), ('but', 69), ('mice', 70), ('an', 71), ('with', 72), ('he', 73), ('case', 74), ('sly', 75), ('this', 76), ('attached', 77), ('up', 78), ('us', 79), ('cat', 80), ('will', 81), ('while', 82), ('venture', 83), ('meet', 84), ('agree', 85), ('manner', 86), ('and', 87), ('is', 88), ('it', 89), ('general', 90), ('at', 91), ('council', 92), ('in', 93), ('mouse', 94), ('if', 95), ('looked', 96), ('make', 97), ('when', 98), ('procured', 99), ('take', 100), ('which', 101), ('you', 102), ('measures', 103), ('a', 104), ('neck', 105), ('i', 106), ('well', 107), ('thought', 108), ('chief', 109), ('easily', 110), ('the', 111)]\n",
      "[[ 0.70708182  0.05904811 -0.85292083 ... -0.12380813  0.42401472\n",
      "   0.7866691 ]\n",
      " [-0.14463772  0.81136515  0.66594539 ...  0.17195009  0.36544339\n",
      "   0.05927553]\n",
      " [-0.35978125 -0.4950007  -0.07144064 ...  0.32410818  0.07836454\n",
      "  -0.68039718]\n",
      " ...\n",
      " [ 0.96711526  0.09028827 -0.98416411 ... -0.75142987 -0.47959537\n",
      "  -0.38218247]\n",
      " [ 0.63607776 -0.51107155 -0.10376463 ... -0.48240197 -0.11824812\n",
      "   0.50935296]\n",
      " [-0.09621922 -0.71201331  0.52079019 ... -0.29315464 -0.59989665\n",
      "   0.47686375]]\n"
     ]
    }
   ],
   "source": [
    "embedding = []\n",
    "dict_list = sorted(dictionary.items(), key = lambda x : x[1])\n",
    "print(dict_list)\n",
    "\n",
    "\n",
    "for i in range(len(dict_list)):\n",
    "    word = dict_list[i]\n",
    "    if word in glove_vocab:\n",
    "        embedding.append( embedding_dict[word] )\n",
    "    else:\n",
    "        temp = np.random.uniform(-1.0, 1.0, emb_dim)\n",
    "        embedding.append(temp)\n",
    "    \n",
    "embedding = np.asarray( embedding )\n",
    "print(embedding)\n",
    "\n",
    "tree = spatial.KDTree(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weights shape', [512, 300])\n",
      "('biases shape', [300])\n",
      "('x', [None, 3])\n",
      "('y', [None, 300])\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs_path = '/tmp/tensorflow/rnn_words'\n",
    "writer = tf.summary.FileWriter(logs_path)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "display_step = 1000\n",
    "n_input = 3\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_hidden,emb_dim]))\n",
    "biases = tf.Variable(tf.random_normal([emb_dim])) \n",
    "print(\"weights shape\", weights.get_shape().as_list())\n",
    "print(\"biases shape\", biases.get_shape().as_list())\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, emb_dim])\n",
    "\n",
    "print(\"x\", x.get_shape().as_list())\n",
    "print(\"y\", y.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, emb_dim]), trainable=True, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, emb_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)\n",
    "embedded_chars = tf.nn.embedding_lookup(W,x)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN( x, weights, biase ):\n",
    "    # reshape input data\n",
    "    x_unstack = tf.unstack(embedded_chars, n_input, 1)\n",
    "    print(\"x_unstack shape\", x_unstack)\n",
    "    \n",
    "    # 2-layer LSTM, each layer has n_hidden units.\n",
    "    # Average Accuracy= 95.20% at 50k iter\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "\n",
    "    # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "    # Average Accuracy= 90.60% 50k iter\n",
    "    # Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above\n",
    "    # rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, _ = rnn.static_rnn(rnn_cell, x_unstack, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights) + biases  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_unstack shape', [<tf.Tensor 'unstack:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'unstack:1' shape=(?, 300) dtype=float32>, <tf.Tensor 'unstack:2' shape=(?, 300) dtype=float32>])\n"
     ]
    }
   ],
   "source": [
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "#cost = tf.reduce_mean(tf.nn.l2_loss(pred-y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding initialized\n",
      "Added in graph\n",
      "['ago', ',', 'the'] - [mice] vs [['mice', 'measures', 'well']]\n",
      "Average Loss= -39640.835993\n",
      "['had', 'a', 'general'] - [council] vs [['council', 'of', 'who']]\n",
      "Average Loss= -43152.547787\n",
      "['to', 'consider', 'what'] - [measures] vs [['measures', 'mice', 'know']]\n",
      "Average Loss= -46982.616101\n",
      "['they', 'could', 'take'] - [to] vs [['to', 'council', 'measures']]\n",
      "Average Loss= -49239.729908\n",
      "['outwit', 'their', 'common'] - [enemy] vs [['enemy', 'mice', 'measures']]\n",
      "Average Loss= -41760.053390\n",
      "[',', 'the', 'cat'] - [.] vs [['.', 'enemy', 'mice']]\n",
      "Average Loss= -34296.628804\n",
      "['some', 'said', 'this'] - [,] vs [[',', 'us', 'is']]\n",
      "Average Loss= 107.107966\n",
      "['and', 'some', 'said'] - [that] vs [['that', ',', 'council']]\n",
      "Average Loss= -19413.444104\n",
      "['but', 'at', 'last'] - [a] vs [['a', '.', 'enemy']]\n",
      "Average Loss= -54721.694952\n",
      "['young', 'mouse', 'got'] - [up] vs [['mice', 'enemy', 'up']]\n",
      "Average Loss= -43363.777782\n",
      "['and', 'said', 'he'] - [had] vs [['had', 'measures', 'mice']]\n",
      "Average Loss= -20196.143356\n",
      "['a', 'proposal', 'to'] - [make] vs [['make', ',', 'that']]\n",
      "Average Loss= -43521.279426\n",
      "[',', 'which', 'he'] - [thought] vs [['thought', 'to', 'measures']]\n",
      "Average Loss= -69591.509357\n",
      "['would', 'meet', 'the'] - [case] vs [['enemy', 'case', '.']]\n",
      "Average Loss= -20726.271696\n",
      "['.', 'you', 'will'] - [all] vs [['all', 'measures', 'had']]\n",
      "Average Loss= -27679.596138\n",
      "['agree', ',', 'said'] - [he] vs [['he', 'a', 'enemy']]\n",
      "Average Loss= -38269.196615\n",
      "[',', 'that', 'our'] - [chief] vs [['chief', 'enemy', 'up']]\n",
      "Average Loss= -41885.330171\n",
      "['danger', 'consists', 'in'] - [the] vs [['the', ',', 'that']]\n",
      "Average Loss= -20151.011843\n",
      "['sly', 'and', 'treacherous'] - [manner] vs [['manner', ',', 'make']]\n",
      "Average Loss= -67059.508824\n",
      "['in', 'which', 'the'] - [enemy] vs [['enemy', 'case', 'up']]\n",
      "Average Loss= -121202.247559\n",
      "['approaches', 'us', '.'] - [now] vs [['now', 'he', 'a']]\n",
      "Average Loss= -62096.956045\n",
      "[',', 'if', 'we'] - [could] vs [['could', 'measures', 'thought']]\n",
      "Average Loss= -102593.440680\n",
      "['receive', 'some', 'signal'] - [of] vs [['of', ',', 'make']]\n",
      "Average Loss= -58677.493339\n",
      "['her', 'approach', ','] - [we] vs [['we', 'could', 'thought']]\n",
      "Average Loss= -63242.406983\n",
      "['could', 'easily', 'escape'] - [from] vs [[',', 'from', 'of']]\n",
      "Average Loss= -28345.156443\n",
      "['her', '.', 'i'] - [venture] vs [['venture', 'council', 'said']]\n",
      "Average Loss= -106035.226281\n",
      "[',', 'therefore', ','] - [to] vs [['to', 'council', 'had']]\n",
      "Average Loss= -167738.614703\n",
      "['propose', 'that', 'a'] - [small] vs [['small', 'a', 'he']]\n",
      "Average Loss= -55899.165254\n",
      "['bell', 'be', 'procured'] - [,] vs [[',', 'make', 'of']]\n",
      "Average Loss= -31399.075078\n",
      "['and', 'attached', 'by'] - [a] vs [['a', 'he', 'small']]\n",
      "Average Loss= -144153.410696\n",
      "['ribbon', 'round', 'the'] - [neck] vs [['to', 'neck', 'council']]\n",
      "Average Loss= -79181.238953\n",
      "['of', 'the', 'cat'] - [.] vs [['enemy', '.', 'mice']]\n",
      "Average Loss= -84239.330774\n",
      "['by', 'this', 'means'] - [we] vs [['we', 'thought', 'could']]\n",
      "Average Loss= -150737.408064\n",
      "['should', 'always', 'know'] - [when] vs [['when', 'a', 'now']]\n",
      "Average Loss= -52206.204461\n",
      "['she', 'was', 'about'] - [,] vs [[',', 'make', 'manner']]\n",
      "Average Loss= -89189.639684\n",
      "['and', 'could', 'easily'] - [retire] vs [['retire', 'we', 'all']]\n",
      "Average Loss= -59722.641128\n",
      "['while', 'she', 'was'] - [in] vs [['in', 'venture', 'measures']]\n",
      "Average Loss= -41756.526247\n",
      "['the', 'neighbourhood', '.'] - [this] vs [[',', 'this', 'council']]\n",
      "Average Loss= -79450.662110\n",
      "['proposal', 'met', 'with'] - [general] vs [['general', 'all', 'retire']]\n",
      "Average Loss= -67313.839776\n",
      "['applause', ',', 'until'] - [an] vs [['enemy', '.', 'an']]\n",
      "Average Loss= -124577.958234\n",
      "['old', 'mouse', 'got'] - [up] vs [['enemy', 'up', '.']]\n",
      "Average Loss= -125849.167141\n",
      "['and', 'said', 'that'] - [is] vs [[',', 'make', 'is']]\n",
      "Average Loss= -109467.131233\n",
      "['all', 'very', 'well'] - [,] vs [[',', 'make', 'is']]\n",
      "Average Loss= -148192.639023\n",
      "['but', 'who', 'is'] - [to] vs [['to', 'neck', 'council']]\n",
      "Average Loss= -271502.073667\n",
      "['bell', 'the', 'cat'] - [?] vs [['?', 'general', 'retire']]\n",
      "Average Loss= -519.062743\n",
      "['the', 'mice', 'looked'] - [at] vs [['enemy', '.', 'at']]\n",
      "Average Loss= -14074.453247\n",
      "['one', 'another', 'and'] - [nobody] vs [['nobody', 'to', 'this']]\n",
      "Average Loss= -130056.946619\n",
      "['spoke', '.', 'then'] - [the] vs [['the', ',', 'manner']]\n",
      "Average Loss= -67596.098818\n",
      "['old', 'mouse', 'said'] - [it] vs [['enemy', 'up', 'mice']]\n",
      "Average Loss= -50631.786016\n",
      "['is', 'easy', 'to'] - [propose] vs [['propose', 'mice', 'an']]\n",
      "Average Loss= -75202.605938\n",
      "Optimization Finished!\n",
      "('Elapsed time: ', '1.92406371693 hr')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    session.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    "    print(\"Embedding initialized\")\n",
    "    \n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "    print(\"Added in graph\")\n",
    "    while step < training_iters:\n",
    "        #print(\"step \" , step+1)\n",
    "        step += 1\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        #print(np.shape(symbols_in_keys)) \n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input])\n",
    "        #print(np.shape(symbols_in_keys)) \n",
    "        \n",
    "        y_id = dictionary[str(training_data[offset+n_input])]\n",
    "        y_emb = embedding[y_id]\n",
    "        #print(\"y_emb shape\" , np.shape(y_emb))\n",
    "        \n",
    "        y_emb = np.reshape(y_emb,(-1,300) )\n",
    "        #print(\"y_emb shape after reshape\", np.shape(y_emb))\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        (3, 1)\n",
    "        (1, 3)\n",
    "        ('y_emb shape', (300,))\n",
    "        ('y_emb shape after reshape', (1, 300))\n",
    "        '''\n",
    "        \n",
    "\n",
    "        _, acc, loss, pred_ = session.run([optimizer, accuracy, cost, pred], feed_dict={x: symbols_in_keys, y: y_emb})\n",
    "        \n",
    "        loss_total += loss\n",
    "#         acc_total += acc\n",
    "#         if (step+1) % display_step == 0:\n",
    "#             print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "#                   \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "#                   \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "#             acc_total = 0\n",
    "#             loss_total = 0\n",
    "#             symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "#             symbols_out = training_data[offset + n_input]\n",
    "#             symbols_out_pred = reverse_dict[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "#             print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "#         step += 1\n",
    "#         offset += (n_input+1)\n",
    "\n",
    "         # display output to show progress\n",
    "  \n",
    "        if (step+1) % display_step ==0:\n",
    "            words_in = [str(training_data[i]) for i in range(offset, offset+n_input)] \n",
    "            target_word = str(training_data[offset+n_input])\n",
    "\n",
    "\n",
    "            nearest_dist,nearest_idx = tree.query(pred_[0],3)\n",
    "            nearest_words = [reverse_dict[idx] for idx in nearest_idx]\n",
    "\n",
    "            print(\"%s - [%s] vs [%s]\" % (words_in, target_word, nearest_words))\n",
    "            print(\"Average Loss= \" + \"{:.6f}\".format(loss_total/display_step))\n",
    "            loss_total=0\n",
    "\n",
    "            step +=1\n",
    "            offset += (n_input+1) \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(\"Run on command line.\")\n",
    "#     #print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "#     #print(\"Point your web browser to: http://localhost:6006/\")\n",
    "#     while True:\n",
    "#         prompt = \"%s words: \" % n_input\n",
    "#         sentence = input(prompt)\n",
    "#         sentence = sentence.strip()\n",
    "#         words = sentence.split(' ')\n",
    "#         print(words)\n",
    "#         if len(words) != n_input:\n",
    "#             continue\n",
    "#         #try:\n",
    "#         symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "#         for i in range(32):\n",
    "#             keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "#             onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "#             onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "#             sentence = \"%s %s\" % (sentence,reverse_dict[onehot_pred_index])\n",
    "#             symbols_in_keys = symbols_in_keys[1:]\n",
    "#             symbols_in_keys.append(onehot_pred_index)\n",
    "#         print(sentence)\n",
    "#         #except:\n",
    "#          #   print(\"Word not in dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ace_env2",
   "language": "python",
   "name": "ace_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
